import torch
import torch.nn as nn
from model.attn import MultiheadAttention


class CausCnnBlock1x1(nn.Module):
	# expansion = 1
	def __init__(self, inplanes, planes, kernel=(1,1), stride=(1,1), padding=(0,0)):
		super(CausCnnBlock1x1, self).__init__()
		self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=kernel, stride=stride, padding=padding, bias=False)

	def forward(self, x):
		out = self.conv1(x)

		return out

class CausCnnBlock(nn.Module):
	""" Function: Basic causal convolutional block
    """
	# expansion = 1
	# def __init__(self, inplanes, planes, kernel=(3,3), stride=(1,1), padding=(1,2), use_res=True, downsample=None):
	def __init__(self, inplanes, planes, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=True):
		super(CausCnnBlock, self).__init__()
		self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=kernel, stride=stride, padding=padding, padding_mode = 'replicate', bias=False)
		self.bn1 = nn.BatchNorm2d(planes)
		self.relu = nn.ReLU(inplace=True)

		self.conv2 = nn.Conv2d(planes, planes, kernel_size=kernel, stride=stride, padding=padding, padding_mode = 'replicate', bias=False)
		
		# conv2 change to 3D conv
        # 这里depth维度kernel=3，stride=1，padding和dilation根据保持维度计算
		# padding对应dilation大小，保持尺寸
		# 3D卷积核，depth=3，height=kernel[0]，width=kernel[1]
		# self.conv2 = nn.Conv3d(in_channels=1,
		# 				 out_channels=1, 
		# 				 kernel_size=(3, kernel[0], kernel[1]),
		# 				 stride=(1, stride[0], stride[1]), 
		# 				 padding=(2, 2, 2), 
		# 				 dilation=(2, 2, 2), 
		# 				 padding_mode='replicate', 
		# 				 bias=False)
		
		self.bn2 = nn.BatchNorm2d(planes)

		# self.downsample = downsample
		self.stride = stride
		self.pad = padding
		self.use_res = use_res

		# self.downsample = nn.Sequential(
		# 	nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),
		# 	nn.BatchNorm2d(planes),
		# )

	def forward(self, x):
		residual = x

		out = self.conv1(x)
		out = self.bn1(out)
		out = self.relu(out)

		# if self.pad[1] !=0:
		# 	out = out[:,:,:,:-self.pad[1]]

		# for 3D conv
		# out = out.unsqueeze(1)  # (B, 1, C, F, T)
		# out = self.conv2(out)
		# out = out.squeeze(1)  # (B, C, F, T)


		out = self.conv2(out)
		out = self.bn2(out)
		# if self.pad[1] != 0:
		# 	out = out[:, :, :, :-self.pad[1]]

		if self.use_res == True:
			# if self.downsample is not None:
			# residual = self.downsample(x)
			out += residual

		out = self.relu(out)

		return out


class CRNN(nn.Module):
	""" Proposed CRNN model
	"""
	def __init__(self,
				 input_dim,
				 output_dim,
					dropout_rate=0.1,
              ):
		super(CRNN, self).__init__()

		cnn_in_dim = input_dim
		cnn_dim = 64
		res_flag = True
		# attn_embed_dim = 256,
		self.cnn = nn.Sequential(
				# Layer 1
                CausCnnBlock(cnn_in_dim, cnn_dim, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=False),

                # MultiheadAttention(input_dim=cnn_dim,
                #                    embed_dim=cnn_dim,
                #                    num_heads=4,
				# 				   residual=False),

                nn.MaxPool2d(kernel_size=(4, 1)),
				# nn.Conv2d(cnn_dim, cnn_dim, kernel_size=(4,1), stride=(4,1), padding=(1,0), bias=False, padding_mode='replicate'),

				# Layer 2
				CausCnnBlock(cnn_dim, cnn_dim, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=res_flag),

				# MultiheadAttention(input_dim=cnn_dim,
                #                    embed_dim=cnn_dim,
                #                    num_heads=4,
				# 				   residual=False),

				nn.MaxPool2d(kernel_size=(2, 1)),
				# nn.Conv2d(cnn_dim, cnn_dim, kernel_size=(2,1), stride=(2,1), padding=(0,0), bias=False, padding_mode='replicate'),

				# Layer 3
				CausCnnBlock(cnn_dim, cnn_dim, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=res_flag),

				# MultiheadAttention(input_dim=cnn_dim,
                #                    embed_dim=cnn_dim,
                #                    num_heads=4,
				# 				   residual=False),

				nn.MaxPool2d(kernel_size=(2, 2)),
				# nn.Conv2d(cnn_dim, cnn_dim, kernel_size=(2,2), stride=(2,2), padding=(0,0), bias=False, padding_mode='replicate'),
				
				# Layer 4
				CausCnnBlock(cnn_dim, cnn_dim, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=res_flag),

				MultiheadAttention(input_dim=cnn_dim,
                                   embed_dim=cnn_dim,
                                   num_heads=4,
								   residual=False),

				nn.MaxPool2d(kernel_size=(2, 2)),
				# nn.Conv2d(cnn_dim, cnn_dim, kernel_size=(2,2), stride=(2,2), padding=(0,0), bias=False, padding_mode='replicate'),

				# Layer 5
				CausCnnBlock(cnn_dim, cnn_dim, kernel=(3,3), stride=(1,1), padding=(1,1), use_res=res_flag),

				# MultiheadAttention(input_dim=cnn_dim,
                #                    embed_dim=cnn_dim,
                #                    num_heads=4,
				# 				   residual=False),

				nn.MaxPool2d(kernel_size=(2, 3)),
				# nn.Conv2d(cnn_dim, cnn_dim, kernel_size=(2,3), stride=(2,3), padding=(0,0), bias=False, padding_mode='replicate'),
            )

		cnn_out_dim = 256
		rnn_in_dim = output_dim
		# rnn_in_dim = 256
		# rnn_hid_dim = 256
		rnn_out_dim = output_dim
		# rnn_bdflag = False
		rnn_bdflag = True
		if rnn_bdflag:
			rnn_ndirection = 2
		else:
			rnn_ndirection = 1
		rnn_hid_dim = rnn_in_dim // rnn_ndirection

		self.rnn_in_fc = nn.Sequential(
			# nn.Dropout(dropout_rate),
            nn.Linear(in_features=cnn_out_dim, out_features=rnn_in_dim),
            nn.ReLU(inplace=True)
        )

		# self.rnn = torch.nn.LSTM(input_size=rnn_in_dim, hidden_size=rnn_hid_dim, num_layers=2, batch_first=True, bias=True, dropout=dropout_rate, bidirectional=rnn_bdflag)

		self.rnn = torch.nn.LSTM(input_size=rnn_in_dim, hidden_size=rnn_hid_dim, num_layers=1, batch_first=True, bias=True, dropout=dropout_rate, bidirectional=rnn_bdflag)

		self.rnn_out_fc = nn.Sequential(
			nn.Dropout(dropout_rate),
			torch.nn.Linear(in_features=rnn_ndirection * rnn_hid_dim, out_features=rnn_out_dim),
			# nn.Tanh(),
		)


	def forward(self, x):
		fea = x
		nb, _, nf, nt = fea.shape
		fea_cnn = self.cnn(fea)
		fea_rnn_in = fea_cnn.view(nb, -1, fea_cnn.size(3))
		fea_rnn_in = fea_rnn_in.permute(0, 2, 1)

		# With Linear Layer before LSTM
		fea_rnn_in = self.rnn_in_fc(fea_rnn_in)
		# # With LSTM Layer
		fea_rnn, _ = self.rnn(fea_rnn_in)
		fea_rnn_out = self.rnn_out_fc(fea_rnn)

		# Without LSTM Layer
		# fea_rnn_fc = self.rnn_fc(fea_rnn_in)

		# print(f'fea_rnn_fc shape: {fea_rnn_fc.shape}')
		return fea_rnn_out



if __name__ == "__main__":
	import torch
	from torchinfo import summary
	# torch.cuda.set_device(1)
	batch_size = 8
	channels = 4
	frequency = 256
	time = int(16000/512) * 10
	input = torch.randn((batch_size, channels, frequency, time)).cuda()
	# input = torch.randn((batch_size, channels, frequency, time))
	net = CRNN(input_dim=4, output_dim=180).cuda()
	# net = CRNN(input_dim=4, output_dim=180)
	ouput = net(input)
	print('# parameters:', sum(param.numel() for param in net.parameters()))
	summary(
		net,
		input_size=(batch_size, channels, frequency, time),  # 输入张量维度
		col_names=["input_size", "output_size", "kernel_size", "num_params", "params_percent"],  # 显示关键信息
		# col_names=["input_size", "output_size", "kernel_size", "mult_adds"],
		depth=3  # 显示模块嵌套层级
	)

